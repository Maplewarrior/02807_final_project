{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from models.builers.retriever import Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset from data/datasets\\fiqa\n",
      "data/datasets\\fiqa\\corpus.jsonl\n"
     ]
    }
   ],
   "source": [
    "from data.dataloader import Data\n",
    "import configparser\n",
    "\n",
    "# load config.ini \n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "data_handler = Data(config)\n",
    "dataset = \"fiqa\"\n",
    "corpus, queries = data_handler.get_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 relevants from the dictionary\n",
      "   0 : [('18850', 1)]\n",
      "   4 : [('196463', 1)]\n",
      "   5 : [('69306', 1)]\n",
      "   6 : [('560251', 1), ('188530', 1), ('564488', 1)]\n",
      "   7 : [('411063', 1)]\n",
      "\n",
      "First query and document from corpus:\n",
      "  Corpus[0]:   {'_id': '3', 'title': '', 'text': \"I'm not saying I don't like the idea of on-the-job training too, but you can't expect the company to do that. Training workers is not their job - they're building software. Perhaps educational systems in the U.S. (or their students) should worry a little about getting marketable skills in exchange for their massive investment in education, rather than getting out with thousands in student debt and then complaining that they aren't qualified to do anything.\", 'metadata': {}}\n",
      "  Query[0]:  {'_id': '0', 'text': 'What is considered a business expense on a business trip?', 'metadata': {}}\n"
     ]
    }
   ],
   "source": [
    "relevants = data_handler.get_relevants(dataset)\n",
    "# print first 5 relevants from the dictionary\n",
    "print(\"First 5 relevants from the dictionary\")\n",
    "for i in list(relevants.keys())[:5]:\n",
    "    print(\"  \", i, \":\", relevants[i])\n",
    "\n",
    "print(\"\\nFirst query and document from corpus:\")\n",
    "print(\"  Corpus[0]:  \", corpus[0])\n",
    "print(\"  Query[0]: \", queries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = corpus[:10]\n",
    "del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\OneDrive - Danmarks Tekniske Universitet\\Masters\\02807 Computational Tools for Data Science\\venvCompTools\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from models.model_loader_helpers import create_models\n",
    "\n",
    "# models_to_create = {\"TF-IDF\": {},\n",
    "#                     \"BM25\": {},\n",
    "#                     \"DPR\": {},\n",
    "#                     \"Crossencoder\": {\"n\":25},\n",
    "#                     \"KMeans\": {\"k\":4},\n",
    "#                     \"CURE\": {\"k\": 2, \"n\": 2, \"shrinkage_fraction\":0.2}}\n",
    "\n",
    "# create_models(documents=documents, dataset_name=dataset, models=models_to_create, save=True)\n",
    "\n",
    "from models.model_loader_helpers import load_models\n",
    "\n",
    "models_to_load = {\"TF-IDF\": {},\n",
    "                    \"BM25\": {},\n",
    "                    \"DPR\": {}}\n",
    "models = load_models(\"fiqa\", models_to_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is considered a business expense on a business trip?', 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data.query import Query\n",
    "\n",
    "queries_ = data_handler.get_queries() # Get queries in the correct format\n",
    "queries = []\n",
    "for rel in relevants.items():\n",
    "    id = rel[0]\n",
    "    rels = [r[0] for r in rel[1]]\n",
    "    query = queries_[id]\n",
    "    queries.append(Query(text=query['text'], id=id, relevant_document_ids=rels))\n",
    "\n",
    "# output an example\n",
    "queries[0].GetQuery(), queries[0].GetNumberOfRelevantDocuments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieveQueryAndGetScore(model: Retriever, query: Query, k: int):\n",
    "    retrieved_documents = model.Lookup(query=query.GetQuery(), k=k)\n",
    "    relevancies = []\n",
    "    for document in retrieved_documents:\n",
    "        if query.IsDocumentRelevant(document):\n",
    "            relevancies.append(True)\n",
    "        else:\n",
    "            relevancies.append(False)\n",
    "    return relevancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocalRank(relevancies):\n",
    "    for i, relevancy in enumerate(relevancies):\n",
    "        if relevancy:\n",
    "            return 1/(i+1)\n",
    "    return 0\n",
    "\n",
    "def meanReciprocalRank(scores):\n",
    "    reciprocal_ranks = []\n",
    "    for score in scores:\n",
    "        reciprocal_ranks.append(reciprocalRank(score))\n",
    "\n",
    "    return sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "\n",
    "def precision(relevancies):\n",
    "    return sum([1 if relevancy else 0 for relevancy in relevancies]) / len(relevancies)\n",
    "\n",
    "def recall(relevancies, query: Query):\n",
    "    return sum([1 if relevancy else 0 for relevancy in relevancies]) / min(len(relevancies), query.GetNumberOfRelevantDocuments())\n",
    "\n",
    "def calculate_metrics(results):\n",
    "    \"\"\" Calculates precision, recall and accuracy based on the results of a query. True or false values are used to indicate whether a document is relevant or not.\n",
    "    \n",
    "    Args:\n",
    "        results (list): A list of lists of booleans, where each list of booleans represents the retrieved documents for a query\n",
    "\n",
    "    Returns:\n",
    "        precision (float): Precision of the results\n",
    "        recall (float): Recall of the results\n",
    "        accuracy (float): Accuracy of the results\n",
    "    \"\"\"\n",
    "    total_true_positives = 0  # Relevant documents correctly retrieved\n",
    "    total_false_positives = 0 # Non-relevant documents incorrectly retrieved\n",
    "    total_false_negatives = 0 # Relevant documents missed\n",
    "    total_true_negatives = 0  # Non-relevant documents correctly not retrieved\n",
    "\n",
    "    for query_results in results:\n",
    "        true_positives = sum(query_results)\n",
    "        false_positives = len(query_results) - true_positives\n",
    "        # Assuming the length of the list is the total number of relevant documents for the query\n",
    "        false_negatives = len(query_results) - true_positives  \n",
    "        # True negatives can't be calculated without knowing the total number of non-relevant documents\n",
    "\n",
    "        total_true_positives += true_positives\n",
    "        total_false_positives += false_positives\n",
    "        total_false_negatives += false_negatives\n",
    "\n",
    "    # Calculating metrics\n",
    "    precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "    recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
    "    accuracy = (total_true_positives + total_true_negatives) / (total_true_positives + total_false_positives + total_false_negatives + total_true_negatives) if (total_true_positives + total_false_positives + total_false_negatives + total_true_negatives) > 0 else 0\n",
    "\n",
    "    mrr = meanReciprocalRank(results)\n",
    "\n",
    "    return precision, recall, accuracy, mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timeFunction(function, **args):\n",
    "    time_before = time.perf_counter()\n",
    "    output = function(**args)\n",
    "    time_after = time.perf_counter()\n",
    "    return time_after - time_before, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {dataset:{}}\n",
    "times = {dataset: {}}\n",
    "for model_name in [\"TF-IDF\", \"BM25\"]:\n",
    "# for model_name in list(models_to_load.keys()):\n",
    "    scores[dataset][model_name] = list()\n",
    "    times[dataset][model_name] = list()\n",
    "    for query in queries:\n",
    "        t_, s_ = timeFunction(retrieveQueryAndGetScore, **{\"model\": models[model_name], \"query\": query, \"k\": query.GetNumberOfRelevantDocuments()})\n",
    "        scores[dataset][model_name].append(s_)\n",
    "        times[dataset][model_name].append(t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, Recall, Accuracy, MRR\n",
      "TF-IDF: (0.0, 0.0, 0.0, 0.0) Avg. time: 0.00014021054545363852\n",
      "BM25: (0.0002158273381294964, 0.0002158273381294964, 0.00010792531568154837, 0.0005454545454545455) , Avg. time: 5.471114545385221e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision, Recall, Accuracy, MRR\")\n",
    "print(\"TF-IDF:\", calculate_metrics(scores[dataset][\"TF-IDF\"]), \"Avg. time:\", sum(times[dataset][\"TF-IDF\"])/len(times[dataset][\"TF-IDF\"]))\n",
    "print(\"BM25:\", calculate_metrics(scores[dataset][\"BM25\"]), \", Avg. time:\", sum(times[dataset][\"BM25\"])/len(times[dataset][\"BM25\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_latex_table(scores, times, caption=\"Experiment results.\", label=\"tab:results-table\"):\n",
    "    \"\"\"Prints a latex table from the scores and times dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        scores (dict): A dictionary containing the scores for each model and dataset\n",
    "        times (dict): A dictionary containing the times for each model and dataset\n",
    "        caption (str, optional): Caption of the table. Defaults to \"Experiment results.\".\n",
    "        label (str, optional): Label of the table. Defaults to \"tab:results-table\".\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"\\\\begin{table}[h]\")\n",
    "    print(\"\\\\begin{tabular}{ll|lllll}\")\n",
    "    print(\"\\\\textbf{Dataset} & \\\\textbf{Models} & \\\\textbf{Time} & \\\\textbf{Accuracy} & \\\\textbf{Precision} & \\\\textbf{Recall} & \\\\textbf{MRR} \\\\\\\\ \\\\hline\")\n",
    "\n",
    "    dataset_names = list(scores.keys())\n",
    "\n",
    "    for dname in dataset_names:\n",
    "        models_names = list(scores[dname].keys())\n",
    "        # Loop through scores for all the models \n",
    "        for i, model_name in enumerate(models_names):\n",
    "            precision, recall, accuracy, mrr = calculate_metrics(scores[dname][model_name])\n",
    "            t_ = sum(times[dname][model_name])/len(times[dname][model_name])\n",
    "\n",
    "            n_dec = 4\n",
    "            # round numbers \n",
    "            precision = round(precision, n_dec)\n",
    "            recall = round(recall, n_dec)\n",
    "            accuracy = round(accuracy, n_dec)\n",
    "            mrr = round(mrr, n_dec)\n",
    "            t_ = round(t_, n_dec)\n",
    "\n",
    "            if i==0:\n",
    "                stri = f\"\\multirow{{{len(models_names)}}}{{*}}{{\\\\rotatebox[origin=c]{{90}}{{{dname}}}}} & {model_name} & {t_} s & {accuracy} & {precision} & {recall} & {mrr} \\\\\\\\\"\n",
    "            else:\n",
    "                stri = f\" & {model_name} & {t_} s & {accuracy} & {precision} & {recall} & {mrr} \\\\\\\\\"\n",
    "            print(stri)    \n",
    "        \n",
    "        print(\"\\\\hline\")\n",
    "\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(f\"\\\\caption{{{caption}}}\")\n",
    "    print(f\"\\\\label{{{label}}}\")\n",
    "    print(\"\\\\end{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\begin{tabular}{ll|lllll}\n",
      "\\textbf{Dataset} & \\textbf{Models} & \\textbf{Time} & \\textbf{Accuracy} & \\textbf{Precision} & \\textbf{Recall} & \\textbf{MRR} \\\\ \\hline\n",
      "\\multirow{2}{*}{\\rotatebox[origin=c]{90}{fiqa}} & TF-IDF & 0.0001 s & 0.0 & 0.0 & 0.0 & 0.0 \\\\\n",
      " & BM25 & 0.0001 s & 0.0001 & 0.0002 & 0.0002 & 0.0005 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{Experiment results.}\n",
      "\\label{tab:results-table}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "create_latex_table(scores,times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiments on all datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! The following has not been tested yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_new_models = False # Set to True to create new models. False to load existing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets: ['quora', 'nfcorpus', 'msmarco', 'trec-covid', 'fiqa']\n"
     ]
    }
   ],
   "source": [
    "from data.dataloader import Data\n",
    "import configparser\n",
    "\n",
    "# load config.ini \n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "all_datasets = list(config[\"DATASETS\"])\n",
    "print(\"All datasets:\", all_datasets)\n",
    "\n",
    "# Store scores and retrieval times in dictionaries\n",
    "scores = {}\n",
    "times = {}\n",
    "\n",
    "for dataset in all_datasets:\n",
    "    print(\"Dataset:\", dataset)\n",
    "\n",
    "    # Get the dataset\n",
    "    corpus, queries = data_handler.get_dataset(dataset)\n",
    "    relevants = data_handler.get_relevants(dataset)\n",
    "    documents = corpus\n",
    "    del corpus\n",
    "    \n",
    "\n",
    "    # Define models with parameters\n",
    "    models_to_create = {\"TF-IDF\": {},\n",
    "                            \"BM25\": {},\n",
    "                            \"DPR\": {},\n",
    "                            \"Crossencoder\": {\"n\":25},\n",
    "                            \"KMeans\": {\"k\":4},\n",
    "                            \"CURE\": {\"k\": 2, \"n\": 2, \"shrinkage_fraction\":0.2}}\n",
    "    \n",
    "    # Either load or create models\n",
    "    if create_new_models:\n",
    "        from models.model_loader_helpers import create_models\n",
    "\n",
    "        models = create_models(documents=documents, dataset_name=dataset, models=models_to_create, save=True)\n",
    "    else:\n",
    "        from models.model_loader_helpers import load_models\n",
    "        \n",
    "        models = load_models(dataset, models_to_create)\n",
    "\n",
    "    # Get queries in the correct format\n",
    "    queries_ = data_handler.get_queries() # Get queries in the correct format\n",
    "    queries = []\n",
    "    for rel in relevants.items():\n",
    "        id = rel[0]\n",
    "        rels = [r[0] for r in rel[1]]\n",
    "        query = queries_[id]\n",
    "        queries.append(Query(text=query['text'], id=id, relevant_document_ids=rels))\n",
    "\n",
    "    # Now calculate the scores\n",
    "    scores[dataset] = {}\n",
    "    times[dataset] = {}\n",
    "    for model_name in list(models_to_create.keys()):\n",
    "        scores[dataset][model_name] = list()\n",
    "        times[dataset][model_name] = list()\n",
    "        for query in queries:\n",
    "            t_, s_ = timeFunction(retrieveQueryAndGetScore, **{\"model\": models[model_name], \"query\": query, \"k\": query.GetNumberOfRelevantDocuments()})\n",
    "            scores[dataset][model_name].append(s_)\n",
    "            times[dataset][model_name].append(t_)\n",
    "        print(f\"{model_name}:\", calculate_metrics(scores[dataset][model_name]), \", Avg. time:\", sum(times[dataset][model_name])/len(times[dataset][model_name]))\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the latex table with the results\n",
    "create_latex_table(scores,times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
