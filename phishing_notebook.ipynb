{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Imports and Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import all necessary helper functions and classes.\n",
    "We also define the device to run the models on (GPU or CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.phishing import LoadPhishingDataset\n",
    "from models.builers.retriever import Retriever\n",
    "from data.dataloader import DataLoader\n",
    "from models.model_loader_helpers import createModels, loadModels\n",
    "from utils.phishing_utils import getPhishingQueries\n",
    "from models.DPR import DPR\n",
    "from utils.metrics_uitls import timeFunction\n",
    "from utils.phishing_utils import calculatePhishingAccuracy, evaluatePhishingByMajorityVote\n",
    "import configparser\n",
    "import torch\n",
    "import os\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Prepare Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define Experiment Configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the configuration of the experiment.\n",
    "Both the datasets to perform the experiment on and the model configurations.\n",
    "\n",
    "Change the load_saved_models variable to True, to load locally saved models, instead of creating them during the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('configs/config.ini')\n",
    "data_loader = DataLoader(config)\n",
    "\n",
    "model_descriptions = {\"TF-IDF\": {},\n",
    "        \"BM25\": {},\n",
    "        \"DPR\": {},\n",
    "        \"Crossencoder\": {\"n\":25}}\n",
    "        #\"KMeans\": {\"k\":4},\n",
    "        #\"CURE\": {\"k\": 2, \"n\": 2, \"shrinkage_fraction\":0.2}}\n",
    "\n",
    "load_saved_models = False\n",
    "\n",
    "embedding_model_name = \"bert-base-uncased\"\n",
    "embedding_index_folder_path = \"indexes\"\n",
    "\n",
    "top_k = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define Function to Pre-compute Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function helps us reduce a lot of computations, by pre computing the embeddings offline and loading them online, instead of computing them multiple times (one time for each model that relies on embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preComputeEmbeddings(dataset: str, \n",
    "                         documents: list[dict], \n",
    "                         embedding_model_name: str, \n",
    "                         embedding_index_folder_path: str):\n",
    "    embedder = DPR(documents, model_name=embedding_model_name)\n",
    "    embedding_index_path = getPreComputedEmbeddingsPath(dataset, embedding_index_folder_path)\n",
    "    embedder.SaveIndex(embedding_index_path)\n",
    "    return embedding_index_path\n",
    "\n",
    "def getPreComputedEmbeddingsPath(dataset: str, embedding_index_folder_path: str):\n",
    "    return os.path.join(embedding_index_folder_path,dataset,\"embedding_index.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Run Experiemnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the experiment itself.\n",
    "We itterate over all datasets and perform retrieval for each query for each model.\n",
    "We then return the score metrics, which are the mean precision, recall, reciprocal rank and time for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runPhishingExperiment( datasets_path: str, \n",
    "                  model_descriptions: dict[str, dict],\n",
    "                  embedding_model_name: str,\n",
    "                  embedding_index_folder_path: str,\n",
    "                  top_k: int):\n",
    "    score_metrics: dict[str, dict[str, float]] = {}\n",
    "    dataset = LoadPhishingDataset(datasets_path)\n",
    "    queries = getPhishingQueries(dataset)\n",
    "    queries = queries[:30]\n",
    "    documents = dataset.GetDocumentDicts()\n",
    "    documents = documents[:25]\n",
    "    if load_saved_models:\n",
    "        models = loadModels(dataset, model_descriptions)\n",
    "    else:\n",
    "        embedding_index_path = preComputeEmbeddings(\n",
    "                            \"phishing\", \n",
    "                            documents,\n",
    "                            embedding_model_name,\n",
    "                            embedding_index_folder_path)\n",
    "        models: dict[str, Retriever] = createModels(documents=documents, \n",
    "                                dataset_name=\"phishing\", \n",
    "                                models=model_descriptions, \n",
    "                                embedding_index_path=embedding_index_path,\n",
    "                                save=True)\n",
    "    for model_name, model in models.items():\n",
    "        preds = []\n",
    "        labels = []\n",
    "        times = []\n",
    "        score_metrics[model_name] = {}\n",
    "        for query in queries:\n",
    "            time, retrieved_documents = timeFunction(model.Lookup, \n",
    "                                                **{\"queries\": [query.getQuery()], \n",
    "                                                \"k\": top_k})\n",
    "            retrieved_labels = [dataset.GetLabelFromId(document.GetId()) for document in retrieved_documents]\n",
    "            pred = evaluatePhishingByMajorityVote(retrieved_labels)\n",
    "            preds.append(pred)\n",
    "            labels.append(query.getLabel())\n",
    "            times.append(time)\n",
    "        score_metrics[model_name][\"accuracy\"] = calculatePhishingAccuracy(preds, labels)\n",
    "        score_metrics[model_name][\"time\"] = sum(times)/len(times)\n",
    "    return score_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPR running on: cuda\n",
      "DENSE RETRIEVER IDX PATH:  None\n",
      "Building embedding index using device cuda. Running this on GPU is strongly adviced!\n",
      "Creating TF-IDF model\n",
      "GetCorpusVocabulary Elapsed: 0.0009987354278564453s\n",
      "GetInverseDocumentFrequencies Elapsed: 0.003000020980834961s\n",
      "GetDocumentsTFIDFVectors Elapsed: 0.018000125885009766s\n",
      "Saving model 'TF-IDF' at: models/pickled_models/phishing/TF-IDF.pickle\n",
      "Creating BM25 model\n",
      "GetCorpusVocabulary Elapsed: 0.0009996891021728516s\n",
      "GetInverseDocumentFrequencies Elapsed: 0.002999544143676758s\n",
      "GetDocumentLengths Elapsed: 0.0010004043579101562s\n",
      "GetDocumentBM25Vectors Elapsed: 0.017000436782836914s\n",
      "Saving model 'BM25' at: models/pickled_models/phishing/BM25.pickle\n",
      "Creating DPR model\n",
      "DPR running on: cuda\n",
      "DENSE RETRIEVER IDX PATH:  indexes\\phishing\\embedding_index.pickle\n",
      "Saving model 'DPR' at: models/pickled_models/phishing/DPR.pickle\n",
      "Crossencoder model\n",
      "DPR running on: cuda\n",
      "DENSE RETRIEVER IDX PATH:  indexes\\phishing\\embedding_index.pickle\n",
      "Saving model 'Crossencoder' at: models/pickled_models/phishing/Crossencoder_n25.pickle\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\micha\\OneDrive\\Skrivebord\\02807_final_project\\phishing_notebook.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/micha/OneDrive/Skrivebord/02807_final_project/phishing_notebook.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m score_metrics \u001b[39m=\u001b[39m runPhishingExperiment(\u001b[39m\"\u001b[39;49m\u001b[39mdata/datasets/Phishing_Email.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/micha/OneDrive/Skrivebord/02807_final_project/phishing_notebook.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                   model_descriptions,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/micha/OneDrive/Skrivebord/02807_final_project/phishing_notebook.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                   embedding_model_name,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/micha/OneDrive/Skrivebord/02807_final_project/phishing_notebook.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                   embedding_index_folder_path,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/micha/OneDrive/Skrivebord/02807_final_project/phishing_notebook.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                   top_k)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/micha/OneDrive/Skrivebord/02807_final_project/phishing_notebook.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(score_metrics)\n",
      "\u001b[1;32mc:\\Users\\micha\\OneDrive\\Skrivebord\\02807_final_project\\phishing_notebook.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/micha/OneDrive/Skrivebord/02807_final_project/phishing_notebook.ipynb#X16sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m score_metrics[model_name] \u001b[39m=\u001b[39m {}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/micha/OneDrive/Skrivebord/02807_final_project/phishing_notebook.ipynb#X16sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m queries:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/micha/OneDrive/Skrivebord/02807_final_project/phishing_notebook.ipynb#X16sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     time, retrieved_documents \u001b[39m=\u001b[39m timeFunction(model\u001b[39m.\u001b[39mLookup, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/micha/OneDrive/Skrivebord/02807_final_project/phishing_notebook.ipynb#X16sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m                                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mqueries\u001b[39m\u001b[39m\"\u001b[39m: [query\u001b[39m.\u001b[39mgetQuery()], \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/micha/OneDrive/Skrivebord/02807_final_project/phishing_notebook.ipynb#X16sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m                                         \u001b[39m\"\u001b[39m\u001b[39mk\u001b[39m\u001b[39m\"\u001b[39m: top_k})\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/micha/OneDrive/Skrivebord/02807_final_project/phishing_notebook.ipynb#X16sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     retrieved_labels \u001b[39m=\u001b[39m [dataset\u001b[39m.\u001b[39mGetLabelFromId(document\u001b[39m.\u001b[39mGetId()) \u001b[39mfor\u001b[39;00m document \u001b[39min\u001b[39;00m retrieved_documents]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/micha/OneDrive/Skrivebord/02807_final_project/phishing_notebook.ipynb#X16sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     pred \u001b[39m=\u001b[39m evaluatePhishingByMajorityVote(retrieved_labels)\n",
      "File \u001b[1;32mc:\\Users\\micha\\OneDrive\\Skrivebord\\02807_final_project\\utils\\metrics_uitls.py:6\u001b[0m, in \u001b[0;36mtimeFunction\u001b[1;34m(function, **args)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtimeFunction\u001b[39m(function, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs):\n\u001b[0;32m      5\u001b[0m     time_before \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m----> 6\u001b[0m     output \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs)\n\u001b[0;32m      7\u001b[0m     time_after \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m time_after \u001b[39m-\u001b[39m time_before, output\n",
      "File \u001b[1;32mc:\\Users\\micha\\OneDrive\\Skrivebord\\02807_final_project\\models\\builers\\retriever.py:60\u001b[0m, in \u001b[0;36mRetriever.Lookup\u001b[1;34m(self, queries, k)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mLookup\u001b[39m(\u001b[39mself\u001b[39m, queries: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m], k: \u001b[39mint\u001b[39m):\n\u001b[0;32m     56\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m    @param queries: The input text(s) to which relevant passages should be found.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m    @param k: The number of relevant passages to retrieve for each input query.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mCalculateScores(queries)\n\u001b[0;32m     61\u001b[0m     ranked_documents \u001b[39m=\u001b[39m [[d \u001b[39mfor\u001b[39;00m _, d \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(\u001b[39mzip\u001b[39m(query_scores, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mGetDocuments()), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m pair: pair[\u001b[39m0\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)] \u001b[39mfor\u001b[39;00m query_scores \u001b[39min\u001b[39;00m scores]\n\u001b[0;32m     62\u001b[0m     \u001b[39mreturn\u001b[39;00m [ranked_document[:\u001b[39mmin\u001b[39m(k, \u001b[39mlen\u001b[39m(ranked_documents[\u001b[39m0\u001b[39m]))] \u001b[39mfor\u001b[39;00m ranked_document \u001b[39min\u001b[39;00m ranked_documents]\n",
      "File \u001b[1;32mc:\\Users\\micha\\OneDrive\\Skrivebord\\02807_final_project\\models\\TFIDF.py:127\u001b[0m, in \u001b[0;36mTFIDF.CalculateScores\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calculate scores for a query\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[39m    np.array: An array of scores for each document\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[39m# Convert the query to a vector\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m query_vector \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mQueryToVector(query)\n\u001b[0;32m    128\u001b[0m \u001b[39m# Maybe we should calculate dot product in batches\u001b[39;00m\n\u001b[0;32m    129\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtfidf_matrix\u001b[39m.\u001b[39mdot(query_vector\u001b[39m.\u001b[39mT)\u001b[39m.\u001b[39mtoarray()\u001b[39m.\u001b[39mflatten()\n",
      "File \u001b[1;32mc:\\Users\\micha\\OneDrive\\Skrivebord\\02807_final_project\\models\\TFIDF.py:102\u001b[0m, in \u001b[0;36mTFIDF.QueryToVector\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Convert a query to a vector\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39m    csr_matrix: A sparse vector representation of the query\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39m# Preprocess the query and get term counts\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m query_terms \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mPreprocessText(query)\n\u001b[0;32m    103\u001b[0m term_freq \u001b[39m=\u001b[39m Counter(query_terms)\n\u001b[0;32m    105\u001b[0m \u001b[39m# Create an empty LIL matrix for the query vector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\micha\\OneDrive\\Skrivebord\\02807_final_project\\models\\TFIDF.py:25\u001b[0m, in \u001b[0;36mTFIDF.PreprocessText\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     23\u001b[0m to_removes \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m!\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m to_remove \u001b[39min\u001b[39;00m to_removes:\n\u001b[1;32m---> 25\u001b[0m     text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39;49mreplace(to_remove, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mlower()\n\u001b[0;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m text\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "score_metrics = runPhishingExperiment(\"data/datasets/Phishing_Email.csv\", \n",
    "                  model_descriptions,\n",
    "                  embedding_model_name,\n",
    "                  embedding_index_folder_path,\n",
    "                  top_k)\n",
    "print(score_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
