{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Imports and Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import all necessary helper functions and classes.\n",
    "We also define the device to run the models on (GPU or CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from models.DPR import DPR\n",
    "import configparser\n",
    "from utils.data_utils import getCorpus, getQueries\n",
    "from data.dataloader import DataLoader\n",
    "from models.model_loader_helpers import createModels, loadModels\n",
    "from utils.metrics_uitls import timeFunction, calculateMetrics\n",
    "from utils.lookup_utils import retrieveQueryAndGetRelevancies\n",
    "from utils.latex_utils import createLatexTable\n",
    "from utils.misc import batch\n",
    "from data.phishing import PhishingDataset\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Prepare Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define Experiment Configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the configuration of the experiment.\n",
    "Both the datasets to perform the experiment on and the model configurations.\n",
    "\n",
    "Change the load_saved_models variable to True, to load locally saved models, instead of creating them during the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('configs/config.ini')\n",
    "datasets = list(config['DATASETS'])\n",
    "data_loader = DataLoader(config)\n",
    "\n",
    "load_saved_models = False\n",
    "\n",
    "embedding_model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\" #'ProsusAI/finbert' #\"bert-base-uncased\"\n",
    "embedding_index_folder_path = \"indexes\"\n",
    "top_k = 100\n",
    "batch_size = 50\n",
    "subset_factors = [1, 2, 4]\n",
    "\n",
    "model_descriptions = {\n",
    "        \"TF-IDF\": {},\n",
    "        \"BM25\": {},\n",
    "        \"DPR\": {},\n",
    "        \"Crossencoder\": {\"n\" : top_k*2},\n",
    "        \"KMeans\": {\"k\":3},\n",
    "        \"CURE\": {\"n\": 25,\n",
    "                \"shrinkage_fraction\" : 0.1,\n",
    "                \"threshold\": 0.35,\n",
    "                \"initial_clusters\": 50,\n",
    "                \"subsample_fraction\": 0.5,\n",
    "                \"similarity_measure\": \"cosine\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define Function to Pre-compute Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function helps us reduce a lot of computations, by pre computing the embeddings offline and loading them online, instead of computing them multiple times (one time for each model that relies on embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preComputeEmbeddings(dataset: str, \n",
    "                         documents: list[dict], \n",
    "                         embedding_model_name: str, \n",
    "                         embedding_index_folder_path: str):\n",
    "    embedder = DPR(documents, model_name=embedding_model_name)\n",
    "    embedding_index_path = getPreComputedEmbeddingsPath(dataset, embedding_index_folder_path)\n",
    "    embedder.SaveIndex(embedding_index_path)\n",
    "    return embedding_index_path\n",
    "\n",
    "def getPreComputedEmbeddingsPath(dataset: str, embedding_index_folder_path: str):\n",
    "    return os.path.join(embedding_index_folder_path,dataset,\"embedding_index.pickle\")\n",
    "\n",
    "def InitializeModels(models: dict, device: str):\n",
    "    for model_name, retriever in models.items():\n",
    "        retriever.device = device # give attribute device to model\n",
    "        if hasattr(retriever, 'model'):\n",
    "            retriever.model.to(device) # send Encoder to device\n",
    "            retriever.index.GetEmbeddingMatrix()\n",
    "            retriever.index.embedding_matrix = retriever.index.embedding_matrix.to(device) # NOTE: This does not work inplace\n",
    "        # if hasattr(retriever, 'crossencoder'):\n",
    "        #     # retriever.crossencoder.to(device)\n",
    "        #     print(\"CE DEVICE:\", retriever.crossencoder.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Run Experiemnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the experiment itself.\n",
    "We itterate over all datasets and perform retrieval for each query for each model.\n",
    "We then return the score metrics, which are the mean precision, recall, reciprocal rank and time for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def runExperiment(data_loader: DataLoader, \n",
    "                  datasets: list[str], \n",
    "                  model_descriptions: dict[str, dict],\n",
    "                  embedding_model_name: str,\n",
    "                  embedding_index_folder_path: str,\n",
    "                  top_k: int):\n",
    "    \n",
    "    score_metrics: dict[str, dict[str, dict[str, float]]] = {}\n",
    "    print(f'{load_saved_models}')\n",
    "    for dataset in datasets:\n",
    "        score_metrics[dataset] = {}\n",
    "        documents, relevant_doc_ids_for_all_queries = getCorpus(data_loader, dataset)\n",
    "        # For testing\n",
    "        documents = documents[:50]\n",
    "        queries = getQueries(data_loader, relevant_doc_ids_for_all_queries)\n",
    "        if load_saved_models:\n",
    "            print(f'Loading saved models!')\n",
    "            models = loadModels(dataset, model_descriptions)\n",
    "            InitializeModels(models, device=device)\n",
    "            print(\"Models loaded!\")\n",
    "        else:\n",
    "            # Compute embeddings if not done already\n",
    "            if not os.path.exists(getPreComputedEmbeddingsPath(dataset, embedding_index_folder_path)):\n",
    "                embedding_index_path = preComputeEmbeddings(dataset, \n",
    "                                documents,\n",
    "                                embedding_model_name,\n",
    "                                embedding_index_folder_path)\n",
    "                print('Finished computing embeddings!')\n",
    "            embedding_index_path = getPreComputedEmbeddingsPath(dataset, embedding_index_folder_path)\n",
    "            print(f'Embedding index path: {embedding_index_path}')\n",
    "            models = createModels(documents=documents, \n",
    "                                   dataset_name=dataset, \n",
    "                                   models=model_descriptions, \n",
    "                                   embedding_index_path=embedding_index_path,\n",
    "                                   save=True)\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            if model_name == 'Crossencoder':\n",
    "                print(f'\\nActual device: {device}\\nCrossencoder target device: {model.crossencoder._target_device}\\nIf these do not match, consider running CreateModel as opposed to LoadModel!\\n')\n",
    "            print(f'Computing results for {model_name}')\n",
    "            results = []\n",
    "            times = []\n",
    "            score_metrics[dataset][model_name] = {}\n",
    "\n",
    "            itt = 0\n",
    "            for query_batch in batch(queries, batch_size):\n",
    "                elapsed, relevancies = timeFunction(retrieveQueryAndGetRelevancies, \n",
    "                                                 **{\"model\": model, \n",
    "                                                    \"queries\": query_batch, \n",
    "                                                    \"k\": top_k})\n",
    "                results.extend(relevancies)\n",
    "                times.append(elapsed)\n",
    "                itt += batch_size\n",
    "                if itt % 500 == 0:\n",
    "                    print(f\"Iter: {itt}/{len(queries)}\")\n",
    "\n",
    "            model_metrics = calculateMetrics(results, queries, subset_factors=subset_factors)\n",
    "            score_metrics[dataset][model_name] = model_metrics\n",
    "            score_metrics[dataset][model_name][\"time\"] = sum(times)/len(times)\n",
    "    return score_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform the acutal experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "\n",
      "Loading dataset from data/datasets/fiqa\n",
      "data/datasets/fiqa/corpus.jsonl\n",
      "Embedding index path: indexes/fiqa/embedding_index.pickle\n",
      "Creating TF-IDF model\n",
      "GetCorpusVocabulary Elapsed: 0.0008649826049804688s\n",
      "GetInverseDocumentFrequencies Elapsed: 0.0017778873443603516s\n",
      "GetDocumentsTFIDFVectors Elapsed: 0.00864410400390625s\n",
      "Saving model 'TF-IDF' at: models/pickled_models/fiqa/TF-IDF.pickle\n",
      "Creating BM25 model\n",
      "GetCorpusVocabulary Elapsed: 0.0008282661437988281s\n",
      "GetInverseDocumentFrequencies Elapsed: 0.0017290115356445312s\n",
      "GetDocumentLengths Elapsed: 0.00034499168395996094s\n",
      "GetDocumentBM25Vectors Elapsed: 0.01258087158203125s\n",
      "Saving model 'BM25' at: models/pickled_models/fiqa/BM25.pickle\n",
      "Creating DPR model\n",
      "Initializing retrieval model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/augusttollerup/Documents/02807_final_project/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/augusttollerup/Documents/02807_final_project/run_experiments.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/augusttollerup/Documents/02807_final_project/run_experiments.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m score_metrics \u001b[39m=\u001b[39m runExperiment(data_loader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/augusttollerup/Documents/02807_final_project/run_experiments.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                               datasets,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/augusttollerup/Documents/02807_final_project/run_experiments.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                               model_descriptions,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/augusttollerup/Documents/02807_final_project/run_experiments.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                               embedding_model_name,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/augusttollerup/Documents/02807_final_project/run_experiments.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                               embedding_index_folder_path,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/augusttollerup/Documents/02807_final_project/run_experiments.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                               top_k\u001b[39m=\u001b[39;49mtop_k)\n",
      "\u001b[1;32m/Users/augusttollerup/Documents/02807_final_project/run_experiments.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/augusttollerup/Documents/02807_final_project/run_experiments.ipynb#X21sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     embedding_index_path \u001b[39m=\u001b[39m getPreComputedEmbeddingsPath(dataset, embedding_index_folder_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/augusttollerup/Documents/02807_final_project/run_experiments.ipynb#X21sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEmbedding index path: \u001b[39m\u001b[39m{\u001b[39;00membedding_index_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/augusttollerup/Documents/02807_final_project/run_experiments.ipynb#X21sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     models \u001b[39m=\u001b[39m createModels(documents\u001b[39m=\u001b[39;49mdocuments, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/augusttollerup/Documents/02807_final_project/run_experiments.ipynb#X21sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m                            dataset_name\u001b[39m=\u001b[39;49mdataset, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/augusttollerup/Documents/02807_final_project/run_experiments.ipynb#X21sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m                            models\u001b[39m=\u001b[39;49mmodel_descriptions, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/augusttollerup/Documents/02807_final_project/run_experiments.ipynb#X21sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m                            embedding_index_path\u001b[39m=\u001b[39;49membedding_index_path,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/augusttollerup/Documents/02807_final_project/run_experiments.ipynb#X21sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m                            save\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/augusttollerup/Documents/02807_final_project/run_experiments.ipynb#X21sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mfor\u001b[39;00m model_name, model \u001b[39min\u001b[39;00m models\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/augusttollerup/Documents/02807_final_project/run_experiments.ipynb#X21sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mif\u001b[39;00m model_name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mCrossencoder\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/02807_final_project/models/model_loader_helpers.py:26\u001b[0m, in \u001b[0;36mcreateModels\u001b[0;34m(documents, dataset_name, models, save, embedding_index_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCreating DPR model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mDPR\u001b[39;00m \u001b[39mimport\u001b[39;00m DPR\n\u001b[0;32m---> 26\u001b[0m     models_[model_name] \u001b[39m=\u001b[39m DPR(documents\u001b[39m=\u001b[39;49mdocuments, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodels[model_name], index_path\u001b[39m=\u001b[39;49membedding_index_path)\n\u001b[1;32m     27\u001b[0m \u001b[39melif\u001b[39;00m model_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCrossencoder\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     28\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCrossencoder model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/02807_final_project/models/DPR.py:8\u001b[0m, in \u001b[0;36mDPR.__init__\u001b[0;34m(self, documents, index_path, model_name, batch_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, documents: \u001b[39mlist\u001b[39m[\u001b[39mdict\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, index_path: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, model_name: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msentence-transformers/multi-qa-mpnet-base-dot-v1\u001b[39m\u001b[39m\"\u001b[39m, batch_size: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m25\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[39msuper\u001b[39;49m(DPR, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(documents, index_path, model_name, batch_size)\n\u001b[1;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDPR running on \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEmbedding model is:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/02807_final_project/models/builers/dense_retriever.py:16\u001b[0m, in \u001b[0;36mDenseRetriever.__init__\u001b[0;34m(self, documents, index_path, model_name, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m index_path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39m# load index if previously saved\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[39msuper\u001b[39m(DenseRetriever, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(documents, index_path)\n\u001b[0;32m---> 16\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mGetEmbeddingMatrix() \u001b[39m# initialize embedding matrix\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39membedding_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39membedding_matrix\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39m# send to device\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEmbedding matrix initialized to \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/02807_final_project/data/embedding_dataset.py:14\u001b[0m, in \u001b[0;36mEmbeddingDataset.GetEmbeddingMatrix\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mGetEmbeddingMatrix\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     13\u001b[0m     emb_matrix \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mfrom_numpy(document\u001b[39m.\u001b[39mGetEmbedding()) \u001b[39mfor\u001b[39;00m document \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocuments]\n\u001b[0;32m---> 14\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_matrix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat(emb_matrix, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "score_metrics = runExperiment(data_loader,\n",
    "                              datasets,\n",
    "                              model_descriptions,\n",
    "                              embedding_model_name,\n",
    "                              embedding_index_folder_path,\n",
    "                              top_k=top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createLatexTable(score_metrics,\n",
    "                 caption=\"Experiment results.\",\n",
    "                 number_of_decimal_points=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
