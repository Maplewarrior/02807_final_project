{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.builers.retriever import Retriever\n",
    "from data.phisherman import LoadPhishingDataset\n",
    "\n",
    "from utils.data_utils import getCorpus, getQueries\n",
    "from models.model_loader_helpers import createModels, loadModels\n",
    "from utils.metrics_uitls import timeFunction, calculateMetrics\n",
    "from utils.lookup_utils import retrieveQueryAndGetRelevancies\n",
    "from utils.latex_utils import createLatexTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = LoadPhishingDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF model\n",
      "GetCorpusVocabulary Elapsed: 5.0884480476379395s\n",
      "GetInverseDocumentFrequencies Elapsed: 21.464350938796997s\n",
      "GetDocumentsTFIDFVectors Elapsed: 105.33353400230408s\n",
      "Saving model 'TF-IDF' at: models/pickled_models/Phisher/TF-IDF.pickle\n",
      "Creating BM25 model\n",
      "GetCorpusVocabulary Elapsed: 5.311795234680176s\n",
      "GetInverseDocumentFrequencies Elapsed: 13.972733974456787s\n",
      "GetDocumentLengths Elapsed: 3.4158408641815186s\n"
     ]
    }
   ],
   "source": [
    "from models.model_loader_helpers import createModels\n",
    "\n",
    "models_to_create = {\"TF-IDF\": {},\n",
    "                    \"BM25\": {}}\n",
    "\n",
    "createModels(documents=dataset, dataset_name=\"Phisher\", models=models_to_create, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model_loader_helpers import loadModels\n",
    "\n",
    "models = loadModels(\"Phisher\", models_to_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.query import Query\n",
    "\n",
    "queries = []\n",
    "\n",
    "for rel in dataset:\n",
    "    id = rel.Id\n",
    "    queries.append(Query(text=rel.text, id=id, relevant_document_ids=dataset.getRelatedDocuments(rel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries[1].getQuery(), queries[0].getNumberOfRelevantDocuments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieveQueryAndGetScore(model: Retriever, query: Query, k: int):\n",
    "    retrieved_documents = model.Lookup(query=query.getQuery(), k=k)\n",
    "    relevancies = []\n",
    "    for document in retrieved_documents:\n",
    "        if query.isDocumentRelevant(document):\n",
    "            relevancies.append(True)\n",
    "        else:\n",
    "            relevancies.append(False)\n",
    "    return relevancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timeFunction(function, **args):\n",
    "    time_before = time.perf_counter()\n",
    "    output = function(**args)\n",
    "    time_after = time.perf_counter()\n",
    "    return time_after - time_before, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    t_, s_ = timeFunction(retrieveQueryAndGetScore, **{\"model\": models['TF-IDF'], \"query\": queries[i], \"k\": queries[i].getNumberOfRelevantDocuments()})\n",
    "    print(t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def major_vote(relevancies: list[bool]) -> str:\n",
    "    \"\"\"Take a list of relevancies returned by the model and get the majority vote for the documents.\n",
    "\n",
    "    Args:\n",
    "        relevancies (list[bool]): List of relevancies returned by the model.\n",
    "\n",
    "    Returns:\n",
    "        str: Majority vote class for the documents.\n",
    "    \"\"\"\n",
    "    return \"Phishing Email\" if relevancies.count(True) > relevancies.count(False) else \"Safe Email\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(preds: list[str], labels: list[str]) -> float:\n",
    "    \"\"\"Take list of predictions and calculate acc\n",
    "\n",
    "    Args:\n",
    "        preds (list[bool]): list of predictions\n",
    "        label ([type]): label\n",
    "    Returns:\n",
    "        accuracy (float): accuracy\n",
    "    \"\"\"\n",
    "    # inefficient but is only run once for each model\n",
    "    correct = 0\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] == labels[i]:\n",
    "            correct += 1\n",
    "    return correct / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "score_metrics = {}\n",
    "# documents, relevant_doc_ids_for_all_queries = getCorpus(data_loader, dataset)\n",
    "# queries = getQueries(data_loader, relevant_doc_ids_for_all_queries)\n",
    "load_saved_models = True\n",
    "\n",
    "if load_saved_models:\n",
    "    models = loadModels(\"Phisher\", models_to_create)\n",
    "else:\n",
    "    embedding_index_path = preComputeEmbeddings(dataset, \n",
    "                        documents,\n",
    "                        embedding_model_name,\n",
    "                        embedding_index_folder_path)\n",
    "    models = createModels(documents=documents, \n",
    "                            dataset_name=dataset, \n",
    "                            models=model_descriptions, \n",
    "                            embedding_index_path=embedding_index_path,\n",
    "                            save=True)\n",
    "for model_name, model in models.items():\n",
    "    results = {\"y_true\": [], \"y_pred\": []}\n",
    "    times = []\n",
    "    score_metrics[model_name] = {}\n",
    "    for query in queries:\n",
    "        # print(query.id, query.text)\n",
    "        timestamp, relevancies = timeFunction(retrieveQueryAndGetRelevancies, \n",
    "                                            **{\"model\": model, \n",
    "                                            \"query\": query, \n",
    "                                            \"k\": 100}) # We retrieve top 100 documents\n",
    "\n",
    "        truth = dataset[query.id].label\n",
    "        pred = major_vote(relevancies)\n",
    "\n",
    "        # Results are in this case just the majority vote of labels for retrieved documents\n",
    "        results[\"y_true\"].append(truth)\n",
    "        results[\"y_pred\"].append(truth)\n",
    "        times.append(timestamp)\n",
    "    accuracy = calculate_accuracy(results[\"y_pred\"], results[\"y_true\"])\n",
    "    score_metrics[model_name][\"accuracy\"] = accuracy\n",
    "    score_metrics[model_name][\"time\"] = sum(times)/len(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
