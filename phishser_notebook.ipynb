{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.builers.retriever import Retriever\n",
    "from data.phisherman import LoadPhishingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not create PhishingEmail from {'Email Text': nan, 'Email Type': 'Phishing Email'}\n",
      "Could not create PhishingEmail from {'Email Text': nan, 'Email Type': 'Phishing Email'}\n",
      "Could not create PhishingEmail from {'Email Text': nan, 'Email Type': 'Phishing Email'}\n",
      "Could not create PhishingEmail from {'Email Text': nan, 'Email Type': 'Phishing Email'}\n",
      "Could not create PhishingEmail from {'Email Text': nan, 'Email Type': 'Phishing Email'}\n",
      "Could not create PhishingEmail from {'Email Text': nan, 'Email Type': 'Phishing Email'}\n",
      "Could not create PhishingEmail from {'Email Text': nan, 'Email Type': 'Phishing Email'}\n",
      "Could not create PhishingEmail from {'Email Text': nan, 'Email Type': 'Phishing Email'}\n",
      "Could not create PhishingEmail from {'Email Text': nan, 'Email Type': 'Phishing Email'}\n",
      "Could not create PhishingEmail from {'Email Text': nan, 'Email Type': 'Phishing Email'}\n",
      "Could not create PhishingEmail from {'Email Text': nan, 'Email Type': 'Phishing Email'}\n",
      "Could not create PhishingEmail from {'Email Text': nan, 'Email Type': 'Phishing Email'}\n",
      "Could not create PhishingEmail from {'Email Text': nan, 'Email Type': 'Phishing Email'}\n",
      "Could not create PhishingEmail from {'Email Text': nan, 'Email Type': 'Phishing Email'}\n",
      "Could not create PhishingEmail from {'Email Text': nan, 'Email Type': 'Phishing Email'}\n",
      "Could not create PhishingEmail from {'Email Text': nan, 'Email Type': 'Phishing Email'}\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = LoadPhishingDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF model\n",
      "GetCorpusVocabulary Elapsed: 2.678410053253174s\n",
      "GetInverseDocumentFrequencies Elapsed: 3.7538979053497314s\n",
      "GetDocumentsTFIDFVectors Elapsed: 29.269556999206543s\n",
      "Saving model 'TF-IDF' at: models/pickled_models/Phisher/TF-IDF.pickle\n",
      "Creating BM25 model\n",
      "GetCorpusVocabulary Elapsed: 2.5160582065582275s\n",
      "GetInverseDocumentFrequencies Elapsed: 3.8616950511932373s\n",
      "GetDocumentLengths Elapsed: 1.2464590072631836s\n",
      "GetDocumentBM25Vectors Elapsed: 35.76833415031433s\n",
      "Saving model 'BM25' at: models/pickled_models/Phisher/BM25.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'TF-IDF': <models.TFIDF.TFIDF at 0x120ae2230>,\n",
       " 'BM25': <models.BM25.BM25 at 0x120ae33a0>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.model_loader_helpers import create_models\n",
    "\n",
    "models_to_create = {\"TF-IDF\": {},\n",
    "                    \"BM25\": {}}\n",
    "\n",
    "create_models(documents=dataset, dataset_name=\"Phisher\", models=models_to_create, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model_loader_helpers import load_models\n",
    "\n",
    "models = load_models(\"Phisher\", models_to_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the other side of * galicismos * * galicismo * is a spanish term which names the improper introduction of french words which are spanish sounding and thus very deceptive to the ear . * galicismo * is often considered to be a * barbarismo * . what would be the term which designates the opposite phenomenon , that is unlawful words of spanish origin which may have crept into french ? can someone provide examples ? thank you joseph m kozono < kozonoj @ gunet . georgetown . edu >',\n",
       " 11321)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data.query import Query\n",
    "\n",
    "queries = []\n",
    "\n",
    "for rel in dataset:\n",
    "    id = rel.Id\n",
    "    queries.append(Query(text=rel.text, id=id, relevant_document_ids=dataset.getRelatedDocuments(rel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries[1].GetQuery(), queries[0].GetNumberOfRelevantDocuments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieveQueryAndGetScore(model: Retriever, query: Query, k: int):\n",
    "    retrieved_documents = model.Lookup(query=query.GetQuery(), k=k)\n",
    "    relevancies = []\n",
    "    for document in retrieved_documents:\n",
    "        if query.IsDocumentRelevant(document):\n",
    "            relevancies.append(True)\n",
    "        else:\n",
    "            relevancies.append(False)\n",
    "    return relevancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocalRank(relevancies):\n",
    "    for i, relevancy in enumerate(relevancies):\n",
    "        if relevancy:\n",
    "            return 1/(i+1)\n",
    "    return 0\n",
    "\n",
    "def meanReciprocalRank(scores):\n",
    "    reciprocal_ranks = []\n",
    "    for score in scores:\n",
    "        reciprocal_ranks.append(reciprocalRank(score))\n",
    "\n",
    "    return sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "\n",
    "def precision(relevancies):\n",
    "    return sum([1 if relevancy else 0 for relevancy in relevancies]) / len(relevancies)\n",
    "\n",
    "def recall(relevancies, query: Query):\n",
    "    return sum([1 if relevancy else 0 for relevancy in relevancies]) / min(len(relevancies), query.GetNumberOfRelevantDocuments())\n",
    "\n",
    "def calculate_metrics(results):\n",
    "    \"\"\" Calculates precision, recall and accuracy based on the results of a query. True or false values are used to indicate whether a document is relevant or not.\n",
    "    \n",
    "    Args:\n",
    "        results (list): A list of lists of booleans, where each list of booleans represents the retrieved documents for a query\n",
    "\n",
    "    Returns:\n",
    "        precision (float): Precision of the results\n",
    "        recall (float): Recall of the results\n",
    "        accuracy (float): Accuracy of the results\n",
    "    \"\"\"\n",
    "    total_true_positives = 0  # Relevant documents correctly retrieved\n",
    "    total_false_positives = 0 # Non-relevant documents incorrectly retrieved\n",
    "    total_false_negatives = 0 # Relevant documents missed\n",
    "    total_true_negatives = 0  # Non-relevant documents correctly not retrieved\n",
    "\n",
    "    for query_results in results:\n",
    "        true_positives = sum(query_results)\n",
    "        false_positives = len(query_results) - true_positives\n",
    "        # Assuming the length of the list is the total number of relevant documents for the query\n",
    "        false_negatives = len(query_results) - true_positives  \n",
    "        # True negatives can't be calculated without knowing the total number of non-relevant documents\n",
    "\n",
    "        total_true_positives += true_positives\n",
    "        total_false_positives += false_positives\n",
    "        total_false_negatives += false_negatives\n",
    "\n",
    "    # Calculating metrics\n",
    "    precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "    recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
    "    accuracy = (total_true_positives + total_true_negatives) / (total_true_positives + total_false_positives + total_false_negatives + total_true_negatives) if (total_true_positives + total_false_positives + total_false_negatives + total_true_negatives) > 0 else 0\n",
    "\n",
    "    mrr = meanReciprocalRank(results)\n",
    "\n",
    "    return precision, recall, accuracy, mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timeFunction(function, **args):\n",
    "    time_before = time.perf_counter()\n",
    "    output = function(**args)\n",
    "    time_after = time.perf_counter()\n",
    "    return time_after - time_before, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryToVector Elapsed: 0.0019011497497558594s\n",
      "CalculateScores Elapsed: 0.03876900672912598s\n",
      "QueryToVector Elapsed: 0.0010249614715576172s\n",
      "CalculateScores Elapsed: 0.02147984504699707s\n",
      "QueryToVector Elapsed: 0.0015799999237060547s\n",
      "CalculateScores Elapsed: 0.03039836883544922s\n",
      "QueryToVector Elapsed: 0.0009589195251464844s\n",
      "CalculateScores Elapsed: 0.023617982864379883s\n",
      "QueryToVector Elapsed: 0.0007722377777099609s\n",
      "CalculateScores Elapsed: 0.018088817596435547s\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    t_, s_ = timeFunction(retrieveQueryAndGetScore, **{\"model\": models['TF-IDF'], \"query\": queries[i], \"k\": queries[i].GetNumberOfRelevantDocuments()})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
