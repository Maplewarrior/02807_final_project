{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from models.builers.retriever import Retriever\n",
    "from models.DPR import DPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting dataset\n",
    "\n",
    "The below code downloads and unzips a specified dataset and saves it to data/datasets/*name_of_dataset*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset from data/datasets/fiqa\n",
      "data/datasets/fiqa/corpus.jsonl\n"
     ]
    }
   ],
   "source": [
    "from data.dataloader import DataLoader\n",
    "import configparser\n",
    "\n",
    "# load config.ini \n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "data_handler = DataLoader(config)\n",
    "dataset = \"fiqa\"\n",
    "corpus, queries = data_handler.get_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 relevants from the dictionary\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'relevant_ids_for_all_queries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/andreasbigom/Documents/dtu/computational_tools/02807_final_project/notebook.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andreasbigom/Documents/dtu/computational_tools/02807_final_project/notebook.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# print first 5 relevants from the dictionary\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andreasbigom/Documents/dtu/computational_tools/02807_final_project/notebook.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFirst 5 relevants from the dictionary\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/andreasbigom/Documents/dtu/computational_tools/02807_final_project/notebook.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(relevant_ids_for_all_queries\u001b[39m.\u001b[39mkeys())[:\u001b[39m5\u001b[39m]:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andreasbigom/Documents/dtu/computational_tools/02807_final_project/notebook.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m\"\u001b[39m, i, \u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m, relevant_ids_for_all_queries[i])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andreasbigom/Documents/dtu/computational_tools/02807_final_project/notebook.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mFirst query and document from corpus:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'relevant_ids_for_all_queries' is not defined"
     ]
    }
   ],
   "source": [
    "relevant_doc_ids_for_queries = data_handler.get_relevants(dataset)\n",
    "# print first 5 relevants from the dictionary\n",
    "print(\"First 5 relevants from the dictionary\")\n",
    "for i in list(relevant_ids_for_all_queries.keys())[:5]:\n",
    "    print(\"  \", i, \":\", relevant_ids_for_all_queries[i])\n",
    "\n",
    "print(\"\\nFirst query and document from corpus:\")\n",
    "print(\"  Corpus[0]:  \", corpus[0])\n",
    "print(\"  Query[0]: \", queries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Queries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build the queries form the downloaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.query import Query\n",
    "\n",
    "queries_ = data_handler.get_queries() # Get queries in the correct format\n",
    "queries: list[Query] = []\n",
    "for query_id, relevant_doc_ids in relevant_doc_ids_for_queries.items():\n",
    "    relevant_doc_ids = [r[0] for r in relevant_doc_ids[1]]\n",
    "    query = queries_[query_id]\n",
    "    queries.append(Query(text=query['text'], \n",
    "                         id=query_id, \n",
    "                         relevant_document_ids=relevant_doc_ids))\n",
    "\n",
    "# output an example\n",
    "queries[0].GetQuery(), queries[0].GetNumberOfRelevantDocuments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE WHEN RUNNNING FULL\n",
    "documents = corpus[:10]\n",
    "del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can save a lot of computations, by pre-calulating the embeddings used in the DRP, DRP Crossencoder, CURE and K-means models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index_path = \"indexes/embedding_index.pickle\"\n",
    "embedding_model = \"bert-base-uncased\"\n",
    "\n",
    "embedder = DPR(documents=documents, model_name=embedding_model)\n",
    "embedder.SaveIndex(embedding_index_path)\n",
    "del embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF model\n",
      "GetCorpusVocabulary Elapsed: 0.0001628398895263672s\n",
      "GetInverseDocumentFrequencies Elapsed: 0.0009119510650634766s\n",
      "GetDocumentsTFIDFVectors Elapsed: 0.003918886184692383s\n",
      "Creating directory: models/pickled_models\n",
      "Saving model 'TF-IDF' at: models/pickled_models/fiqa/TF-IDF.pickle\n",
      "Creating BM25 model\n",
      "GetCorpusVocabulary Elapsed: 0.0001552104949951172s\n",
      "GetInverseDocumentFrequencies Elapsed: 0.0003941059112548828s\n",
      "GetDocumentLengths Elapsed: 7.390975952148438e-05s\n",
      "GetDocumentBM25Vectors Elapsed: 0.0024192333221435547s\n",
      "Saving model 'BM25' at: models/pickled_models/fiqa/BM25.pickle\n",
      "Creating DPR model\n",
      "Saving model 'DPR' at: models/pickled_models/fiqa/DPR.pickle\n",
      "Crossencoder model\n",
      "Saving model 'Crossencoder' at: models/pickled_models/fiqa/Crossencoder_n25.pickle\n",
      "KMeans model\n",
      "Saving model 'KMeans' at: models/pickled_models/fiqa/KMeans_k4.pickle\n",
      "CURE model\n",
      "Saving model 'CURE' at: models/pickled_models/fiqa/CURE_k2_n2_shrinkage_fraction0.2.pickle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'TF-IDF': <models.TFIDF.TFIDF at 0x29af4d510>,\n",
       " 'BM25': <models.BM25.BM25 at 0x29c31de90>,\n",
       " 'DPR': <models.DPR.DPR at 0x29c2f6bd0>,\n",
       " 'Crossencoder': <models.DPR_crossencoder.DPRCrossencoder at 0x29c37b650>,\n",
       " 'KMeans': <models.k_means.KMeans at 0x2cd60c510>,\n",
       " 'CURE': <models.CURE.CURE at 0x2d09c3250>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.model_loader_helpers import create_models\n",
    "\n",
    "models_to_create = {\"TF-IDF\": {},\n",
    "                    \"BM25\": {},\n",
    "                    \"DPR\": {},\n",
    "                    \"Crossencoder\": {\"n\":25},\n",
    "                    \"KMeans\": {\"k\":4},\n",
    "                    \"CURE\": {\"k\": 2, \"n\": 2, \"shrinkage_fraction\":0.2}}\n",
    "\n",
    "create_models(documents=documents, \n",
    "              dataset_name=dataset, \n",
    "              models=models_to_create, \n",
    "              save=True,\n",
    "              embedding_index_path=embedding_index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model_loader_helpers import load_models\n",
    "\n",
    "models_to_load = {\"TF-IDF\": {},\n",
    "                    \"BM25\": {},\n",
    "                    \"DPR\": {}}\n",
    "models = load_models(\"fiqa\", models_to_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieveQueryAndGetRelevancies(model: Retriever, query: Query, k: int):\n",
    "    retrieved_documents = model.Lookup(query=query.GetQuery(), k=k)\n",
    "    relevancies = []\n",
    "    for document in retrieved_documents:\n",
    "        if query.IsDocumentRelevant(document):\n",
    "            relevancies.append(True)\n",
    "        else:\n",
    "            relevancies.append(False)\n",
    "    return relevancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocalRank(relevancies):\n",
    "    for i, relevancy in enumerate(relevancies):\n",
    "        if relevancy:\n",
    "            return 1/(i+1)\n",
    "    return 0\n",
    "\n",
    "def meanReciprocalRank(scores):\n",
    "    reciprocal_ranks = []\n",
    "    for score in scores:\n",
    "        reciprocal_ranks.append(reciprocalRank(score))\n",
    "\n",
    "    return sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "\n",
    "def precision(relevancies):\n",
    "    return sum([1 if relevancy else 0 for relevancy in relevancies]) / len(relevancies)\n",
    "\n",
    "def recall(relevancies, query: Query):\n",
    "    return sum([1 if relevancy else 0 for relevancy in relevancies]) / min(len(relevancies), query.GetNumberOfRelevantDocuments())\n",
    "\n",
    "def calculate_metrics(results):\n",
    "    \"\"\" Calculates precision, recall and accuracy based on the results of a query. True or false values are used to indicate whether a document is relevant or not.\n",
    "    \n",
    "    Args:\n",
    "        results (list): A list of lists of booleans, where each list of booleans represents the retrieved documents for a query\n",
    "\n",
    "    Returns:\n",
    "        precision (float): Precision of the results\n",
    "        recall (float): Recall of the results\n",
    "        accuracy (float): Accuracy of the results\n",
    "    \"\"\"\n",
    "    total_true_positives = 0  # Relevant documents correctly retrieved\n",
    "    total_false_positives = 0 # Non-relevant documents incorrectly retrieved\n",
    "    total_false_negatives = 0 # Relevant documents missed\n",
    "    total_true_negatives = 0  # Non-relevant documents correctly not retrieved\n",
    "\n",
    "    for query_results in results:\n",
    "        true_positives = sum(query_results)\n",
    "        false_positives = len(query_results) - true_positives\n",
    "        # Assuming the length of the list is the total number of relevant documents for the query\n",
    "        false_negatives = len(query_results) - true_positives  \n",
    "        # True negatives can't be calculated without knowing the total number of non-relevant documents\n",
    "\n",
    "        total_true_positives += true_positives\n",
    "        total_false_positives += false_positives\n",
    "        total_false_negatives += false_negatives\n",
    "\n",
    "    # Calculating metrics\n",
    "    precision = total_true_positives / (total_true_positives + total_false_positives) if (total_true_positives + total_false_positives) > 0 else 0\n",
    "    recall = total_true_positives / (total_true_positives + total_false_negatives) if (total_true_positives + total_false_negatives) > 0 else 0\n",
    "    accuracy = (total_true_positives + total_true_negatives) / (total_true_positives + total_false_positives + total_false_negatives + total_true_negatives) if (total_true_positives + total_false_positives + total_false_negatives + total_true_negatives) > 0 else 0\n",
    "\n",
    "    mrr = meanReciprocalRank(results)\n",
    "\n",
    "    return precision, recall, accuracy, mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timeFunction(function, **args):\n",
    "    time_before = time.perf_counter()\n",
    "    output = function(**args)\n",
    "    time_after = time.perf_counter()\n",
    "    return time_after - time_before, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {dataset:{}}\n",
    "times = {dataset: {}}\n",
    "for model_name in [\"TF-IDF\", \"BM25\", \"DPR\"]:\n",
    "# for model_name in list(models_to_load.keys()):\n",
    "    scores[dataset][model_name] = list()\n",
    "    times[dataset][model_name] = list()\n",
    "    for query in queries[:1000]:\n",
    "        t_, s_ = timeFunction(retrieveQueryAndGetRelevancies, **{\"model\": models[model_name], \"query\": query, \"k\": query.GetNumberOfRelevantDocuments()})\n",
    "        scores[dataset][model_name].append(s_)\n",
    "        times[dataset][model_name].append(t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, Recall, Accuracy, MRR\n",
      "TF-IDF: (0.0, 0.0, 0.0, 0.0) Avg. time: 7.994100000587423e-05\n",
      "BM25: (0.0, 0.0, 0.0, 0.0) , Avg. time: 4.232699999647593e-05\n",
      "DPR: (0.0, 0.0, 0.0, 0.0) , Avg. time: 0.027545723999980964\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision, Recall, Accuracy, MRR\")\n",
    "print(\"TF-IDF:\", calculate_metrics(scores[dataset][\"TF-IDF\"]), \"Avg. time:\", sum(times[dataset][\"TF-IDF\"])/len(times[dataset][\"TF-IDF\"]))\n",
    "print(\"BM25:\", calculate_metrics(scores[dataset][\"BM25\"]), \", Avg. time:\", sum(times[dataset][\"BM25\"])/len(times[dataset][\"BM25\"]))\n",
    "print(\"DPR:\", calculate_metrics(scores[dataset][\"DPR\"]), \", Avg. time:\", sum(times[dataset][\"DPR\"])/len(times[dataset][\"DPR\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_latex_table(scores, times, caption=\"Experiment results.\", label=\"tab:results-table\"):\n",
    "    \"\"\"Prints a latex table from the scores and times dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        scores (dict): A dictionary containing the scores for each model and dataset\n",
    "        times (dict): A dictionary containing the times for each model and dataset\n",
    "        caption (str, optional): Caption of the table. Defaults to \"Experiment results.\".\n",
    "        label (str, optional): Label of the table. Defaults to \"tab:results-table\".\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"\\\\begin{table}[h]\")\n",
    "    print(\"\\\\begin{tabular}{ll|lllll}\")\n",
    "    print(\"\\\\textbf{Dataset} & \\\\textbf{Models} & \\\\textbf{Time} & \\\\textbf{Accuracy} & \\\\textbf{Precision} & \\\\textbf{Recall} & \\\\textbf{MRR} \\\\\\\\ \\\\hline\")\n",
    "\n",
    "    dataset_names = list(scores.keys())\n",
    "\n",
    "    for dname in dataset_names:\n",
    "        models_names = list(scores[dname].keys())\n",
    "        # Loop through scores for all the models \n",
    "        for i, model_name in enumerate(models_names):\n",
    "            precision, recall, accuracy, mrr = calculate_metrics(scores[dname][model_name])\n",
    "            t_ = sum(times[dname][model_name])/len(times[dname][model_name])\n",
    "\n",
    "            n_dec = 4\n",
    "            # round numbers \n",
    "            precision = round(precision, n_dec)\n",
    "            recall = round(recall, n_dec)\n",
    "            accuracy = round(accuracy, n_dec)\n",
    "            mrr = round(mrr, n_dec)\n",
    "            t_ = round(t_, n_dec)\n",
    "\n",
    "            if i==0:\n",
    "                stri = f\"\\multirow{{{len(models_names)}}}{{*}}{{\\\\rotatebox[origin=c]{{90}}{{{dname}}}}} & {model_name} & {t_} s & {accuracy} & {precision} & {recall} & {mrr} \\\\\\\\\"\n",
    "            else:\n",
    "                stri = f\" & {model_name} & {t_} s & {accuracy} & {precision} & {recall} & {mrr} \\\\\\\\\"\n",
    "            print(stri)    \n",
    "        \n",
    "        print(\"\\\\hline\")\n",
    "\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(f\"\\\\caption{{{caption}}}\")\n",
    "    print(f\"\\\\label{{{label}}}\")\n",
    "    print(\"\\\\end{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\begin{tabular}{ll|lllll}\n",
      "\\textbf{Dataset} & \\textbf{Models} & \\textbf{Time} & \\textbf{Accuracy} & \\textbf{Precision} & \\textbf{Recall} & \\textbf{MRR} \\\\ \\hline\n",
      "\\multirow{2}{*}{\\rotatebox[origin=c]{90}{fiqa}} & TF-IDF & 0.0001 s & 0.0 & 0.0001 & 0.0001 & 0.0001 \\\\\n",
      " & BM25 & 0.0 s & 0.0001 & 0.0002 & 0.0002 & 0.0005 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{Experiment results.}\n",
      "\\label{tab:results-table}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "create_latex_table(scores,times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiments on all datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! The following has not been tested yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_new_models = False # Set to True to create new models. False to load existing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataloader import DataLoader\n",
    "import configparser\n",
    "\n",
    "# load config.ini \n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "all_datasets = list(config[\"DATASETS\"])\n",
    "print(\"All datasets:\", all_datasets)\n",
    "\n",
    "# Store scores and retrieval times in dictionaries\n",
    "scores = {}\n",
    "times = {}\n",
    "\n",
    "for dataset in all_datasets:\n",
    "    print(\"Dataset:\", dataset)\n",
    "\n",
    "    # Get the dataset\n",
    "    corpus, queries = data_handler.get_dataset(dataset)\n",
    "    relevant_ids_for_all_queries = data_handler.get_relevants(dataset)\n",
    "    documents = corpus\n",
    "    del corpus\n",
    "    \n",
    "\n",
    "    # Define models with parameters\n",
    "    models_to_create = {\"TF-IDF\": {},\n",
    "                            \"BM25\": {},\n",
    "                            \"DPR\": {},\n",
    "                            \"Crossencoder\": {\"n\":25},\n",
    "                            \"KMeans\": {\"k\":4},\n",
    "                            \"CURE\": {\"k\": 2, \"n\": 2, \"shrinkage_fraction\":0.2}}\n",
    "    \n",
    "    # Either load or create models\n",
    "    if create_new_models:\n",
    "        from models.model_loader_helpers import create_models\n",
    "\n",
    "        models = create_models(documents=documents, dataset_name=dataset, models=models_to_create, save=True)\n",
    "    else:\n",
    "        from models.model_loader_helpers import load_models\n",
    "        \n",
    "        models = load_models(dataset, models_to_create)\n",
    "\n",
    "    # Get queries in the correct format\n",
    "    queries_ = data_handler.get_queries() # Get queries in the correct format\n",
    "    queries = []\n",
    "    for rel in relevant_ids_for_all_queries.items():\n",
    "        id = rel[0]\n",
    "        rels = [r[0] for r in rel[1]]\n",
    "        query = queries_[id]\n",
    "        queries.append(Query(text=query['text'], id=id, relevant_document_ids=rels))\n",
    "\n",
    "    # Now calculate the scores\n",
    "    scores[dataset] = {}\n",
    "    times[dataset] = {}\n",
    "    for model_name in list(models_to_create.keys()):\n",
    "        scores[dataset][model_name] = list()\n",
    "        times[dataset][model_name] = list()\n",
    "        for query in queries:\n",
    "            t_, s_ = timeFunction(retrieveQueryAndGetRelevancies, **{\"model\": models[model_name], \"query\": query, \"k\": query.GetNumberOfRelevantDocuments()})\n",
    "            scores[dataset][model_name].append(s_)\n",
    "            times[dataset][model_name].append(t_)\n",
    "        print(f\"{model_name}:\", calculate_metrics(scores[dataset][model_name]), \", Avg. time:\", sum(times[dataset][model_name])/len(times[dataset][model_name]))\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the latex table with the results\n",
    "create_latex_table(scores,times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
